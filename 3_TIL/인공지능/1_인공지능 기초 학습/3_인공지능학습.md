Recap - AI system

- 입력 -> 특정 추출 & 분류 (AI system) -> 출력

Recap - Basic model

- f(x) = Wx + b

- 학습 가능한 파라미터

Recap - 학습

- 경험 사례 (i.e., data)

- 모델 (e.g., 인공신경망)

- 평가 기준

- Supervised learning - Annotation을 통한 학습

Recap - Parameter 학습 (최적화)

- 경사 하강법 (Gradient descent method)

---

뉴럴넷의 부활

- Deep Convolution Neural Networks
  
  - 기존 2011년 방법에 비해 압도적인 성능 차
  
  - 지속적인 발전으로 2015년 사람의 인식 능력을 뛰어 넘음

---

첫 번째 재탄생 - Convolutional Neural Networks (CNN)

- LeNet-5 [Lecun et al., Proceedings of the IEEE 1998]
  
  - 첫 번재 컨볼루션 기반 뉴럴넷 by Yann LeCun, 1998
  
  - 우편번호 인식에 큰 성공을 거둠

ConvNet의 부활

- 과거와의 차이?
  
  - Algorithm
  
  - Data
  
  - Computation
  
  - 오픈소스

- ACM Turing Award 2018 - 컴퓨터 분야의 노벨상
  
  - Yann LeCun
  
  - Geoffrey Hinton
  
  - Yoshua Bengio

### CNN vs. Fully-Connected NN

- Convolution 뉴런은 Perceptron의 일반화 된 형태
  
  - Local 연산을 학습
  
  - Parameter 공유

---

CNN의 구성 layers

- Convolution layer

- Pooling layer

- Activation function

---

Convolution layer

- 출력 size : (N - F) / stride + 1
  
  - ex) N = 7, F = 3:
    
    - stride 1 => (7 - 3)/1 + 1 = 5
    
    - stride 2 => (7 - 3)/2 + 1 = 3
    
    - stride 3 => (7 - 3)/3 + 1 = 2.33

- 이런 필터 문제 해결 위한
  
  - Zero padding

Pooling layer

- "pooling"을 통해 해결이 가능

- Max pooling

Activation function

- Identity (선형)

- Binary step

- Logistic

- TanH

- Piecewise Linear Unit(PLU)

- Rectified linear unit(ReLU) : 최근 많이 사용함

---

CNN은 어떤 지식을 학습하는가?

- 계층적 특징 표현
  
  - Conv. 레이어의 적층을 통해 자연스럽게 유도된 특징

AlexNet

- 전체적인 architecture:
  
  - Conv - Pool - LRN - Conv - Pool - LRN - Conv - Conv - Conv - Pool - FC - FC - FC

- LRN보다 요즘엔 Batch Normalization(배치의 통계치를 통한 정규화) 사용함 

VGGNet

- 더 깊은 아키텍쳐
  
  - 16 and 19 layers

- 간결한 아키텍쳐
  
  - LRN이 없어짐
  
  - 3X3 conv filters, 2X2 max pooling만 사용

- 더 좋은 퍼포먼스
  
  - ImageNet challenge 대회에서 AlexNet보다 획기적으로 좋은 성능을 보임

- 더 좋아진 일반화 성능
  
  - Fine tuning없이 다른 방법으로 일반화를 더 잘하게 됨

---

Degradation problem

- 네트워크가 깊어짐에 따라, 정확도가 더 빠르게 수렴하지만 성능이 나빠짐

- 이것은 overfitting의 문제가 아니라. 문제는 최적화에 있음

---

최적화 해결을 위해 ResNet이 나옴!
















